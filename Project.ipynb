{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3834\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(columns=[\"Tweet_index\",\"Label\",\"Tweet_Text\"]);\n",
    "txt_file=r\"SemEval2018-T3-train-taskA_emoji_ironyHashtags.txt\"\n",
    "#csv_file=r\"data11.csv\"\n",
    "i=0\n",
    "with open(txt_file) as f:\n",
    "    for line in f:\n",
    "        #print(line)\n",
    "        if i==0:\n",
    "            i=i+1\n",
    "            continue\n",
    "        df.loc[i]=[text for text in line.split(\"\\t\")]\n",
    "        i=i+1\n",
    "df.to_csv('input_data.csv', sep=' ',index=False)\n",
    "tweets=df[\"Tweet_Text\"]\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import ne_chunk\n",
    "input_txt=\"\"\n",
    "for line in tweets:\n",
    "    #print(line)\n",
    "    de_emojize=emoji.demojize(line)\n",
    "    commonurl_str = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', 'http://someurl', de_emojize)\n",
    "    replace_username=re.sub('(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)','@someuser',commonurl_str)\n",
    "  #  puncutation_flooding=re.sub(r'['+string.punctuation+']+', ' ',replace_username)\n",
    "    input_txt+=replace_username\n",
    "#print(input_txt)\n",
    "input_tokenized=nltk.word_tokenize(input_txt)\n",
    "#print(input_tokenized)\n",
    "input_posTag=nltk.pos_tag(input_tokenized)\n",
    "#print(input_posTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_flooding(str):\n",
    "    l = len(str)\n",
    "    count=0\n",
    "    for i in range(l):\n",
    "        cur_count=1\n",
    "        if str[i] in string.punctuation:\n",
    "            for j in range(i+1,l):\n",
    "                if str[j] not in string.punctuation or str[j] in ['/','#','.']:\n",
    "                    break\n",
    "                else:\n",
    "                    cur_count+=1\n",
    "        if cur_count > count: \n",
    "            count = cur_count \n",
    "    if count >2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_flooding(str):\n",
    "    l = len(str)\n",
    "    count=0\n",
    "    for i in range(l):\n",
    "        cur_count=1\n",
    "        if str[i].isalpha and str[i] not in [' '] and str[i] not in string.punctuation :\n",
    "             for j in range(i+1,l):\n",
    "                if not str[j] == str[i] :\n",
    "                    break\n",
    "                else:\n",
    "                    cur_count+=1\n",
    "        if cur_count > count: \n",
    "            count = cur_count \n",
    "    if count >2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "lexfeatures=[]\n",
    "i=0\n",
    "for line in input_txt.split('\\n'):\n",
    "    if(len(line)>0):\n",
    "        hashCount=line.count(\"#\")\n",
    "        ispunctflooded= punct_flooding(line)\n",
    "        ischarflooded=char_flooding(line)\n",
    "        emojislist=re.findall(r':[\\w]+:',line)\n",
    "        emojiCount=len(emojislist)\n",
    "        tweetLength=len(line)\n",
    "        wordsList= line.split(' ')\n",
    "        numberOfWords=len(wordsList)\n",
    "        hashtagWordRatio= round(hashCount/numberOfWords,2)\n",
    "        #print(emojislist)\n",
    "        #print(emojiCount)\n",
    "        lexfeatures.insert(i,{'Character_Flooding':ischarflooded,'Punctutation_Flooding':ispunctflooded,'Hashtag_Frequency':hashCount,'Hashtag_to_word_ratio':hashtagWordRatio,'Emoticon_Count':emojiCount,'Tweet_length':tweetLength})\n",
    "        i=i+1\n",
    "df = pd.DataFrame(lexfeatures)\n",
    "df.to_csv('outfiles/lexFeatures.csv', sep='\\t',index=False)\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCount(dict11):\n",
    "    dict13={}\n",
    "    for key, value in dict11.items():\n",
    "        if value>=2:\n",
    "            value=2\n",
    "        dict13[key]=value\n",
    "    return dict13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "i=CC_count=0\n",
    "#count11=[]\n",
    "syntacticfeatures_count=[]\n",
    "syntacticfeatures_presence=[]\n",
    "syntacticfeatures_greater2=[]\n",
    "for line in input_txt.split('\\n'):\n",
    "    if(len(line)>0):\n",
    "        count= Counter([j for i,j in pos_tag(word_tokenize(line))])\n",
    "        #test12=ne_chunk(pos_tag(word_tokenize(line)))\n",
    "        #print(test12)\n",
    "        list11=sorted(count.most_common())\n",
    "        dict11=dict(list11)\n",
    "        dict13=getCount(dict11)\n",
    "        dict12=dict.fromkeys(dict11,1)\n",
    "        #count11=getabsoluteCount(dict11,count11)\n",
    "        syntacticfeatures_count.insert(i,{'CC':dict11.get('CC',0),'JJ':dict11.get('JJ',0),'JJR':dict11.get('JJR',0),'JJS':dict11.get('JJS',0),'MD':dict11.get('MD',0),'NN':dict11.get('NN',0),'NNP':dict11.get('NNP',0),'NNS':dict11.get('NNS',0),'NNPS':dict11.get('NNPS',0),'PRP':dict11.get('PRP',0),'POS':dict11.get('POS',0),'RB':dict11.get('RB',0),'PRP$':dict11.get('PRP$',0),'RBR':dict11.get('RBR',0),'RBS':dict11.get('RBS',0),'UH':dict11.get('UH',0),'VB':dict11.get('VB',0),'VBD':dict11.get('VBD',0),'VBG':dict11.get('VBG',0),'VBP':dict11.get('VBP',0),'VBN':dict11.get('VBN',0),'VBZ':dict11.get('VBZ',0),'WDT':dict11.get('WDT',0),'WP':dict11.get('WP',0),'WP$':dict11.get('WP$',0),'WRB':dict11.get('WRB',0)})\n",
    "        syntacticfeatures_presence.insert(i,{'CC':dict12.get('CC',0),'JJ':dict12.get('JJ',0),'JJR':dict12.get('JJR',0),'JJS':dict12.get('JJS',0),'MD':dict12.get('MD',0),'NN':dict12.get('NN',0),'NNP':dict12.get('NNP',0),'NNS':dict12.get('NNS',0),'NNPS':dict12.get('NNPS',0),'PRP':dict12.get('PRP',0),'POS':dict12.get('POS',0),'RB':dict12.get('RB',0),'PRP$':dict12.get('PRP$',0),'RBR':dict12.get('RBR',0),'RBS':dict12.get('RBS',0),'UH':dict12.get('UH',0),'VB':dict12.get('VB',0),'VBD':dict12.get('VBD',0),'VBG':dict12.get('VBG',0),'VBP':dict12.get('VBP',0),'VBN':dict12.get('VBN',0),'VBZ':dict12.get('VBZ',0),'WDT':dict12.get('WDT',0),'WP':dict12.get('WP',0),'WP$':dict12.get('WP$',0),'WRB':dict12.get('WRB',0)})\n",
    "        syntacticfeatures_greater2.insert(i,{'CC':dict13.get('CC',0),'JJ':dict13.get('JJ',0),'JJR':dict13.get('JJR',0),'JJS':dict13.get('JJS',0),'MD':dict13.get('MD',0),'NN':dict13.get('NN',0),'NNP':dict13.get('NNP',0),'NNS':dict13.get('NNS',0),'NNPS':dict13.get('NNPS',0),'PRP':dict13.get('PRP',0),'POS':dict13.get('POS',0),'RB':dict13.get('RB',0),'PRP$':dict13.get('PRP$',0),'RBR':dict13.get('RBR',0),'RBS':dict13.get('RBS',0),'UH':dict13.get('UH',0),'VB':dict13.get('VB',0),'VBD':dict13.get('VBD',0),'VBG':dict13.get('VBG',0),'VBP':dict13.get('VBP',0),'VBN':dict13.get('VBN',0),'VBZ':dict13.get('VBZ',0),'WDT':dict13.get('WDT',0),'WP':dict13.get('WP',0),'WP$':dict13.get('WP$',0),'WRB':dict13.get('WRB',0)})\n",
    "        i=i+1\n",
    "df = pd.DataFrame(syntacticfeatures_count)\n",
    "df1=pd.DataFrame(syntacticfeatures_presence)\n",
    "df2=pd.DataFrame(syntacticfeatures_greater2)\n",
    "df.to_csv('outfiles/synFeatures_countofTagPerTweet.csv', sep='\\t',index=False)\n",
    "df1.to_csv('outfiles/synFeatures_presenceTaginTweet.csv', sep='\\t',index=False)\n",
    "df2.to_csv('outfiles/synFeatures_greaterthan2inTweet.csv', sep='\\t',index=False)\n",
    "#list11=sorted(count.iteritems())\n",
    "#print(list11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
